# ğŸ“Š Data Quality Checker  
A professional, modular, and extensible **data validation toolkit** with both **CLI** and **Streamlit UI** for analyzing dataset quality, detecting issues, and generating structured reports.  
This project is designed for **Data Engineers, ML Engineers, and Data Scientists** who need to ensure data integrity before ingestion, modeling, or deployment.

---

## ğŸš€ Features

### ğŸ” Automated Data Validation  
- Missing value detection  
- Duplicate row detection  
- Schema validation  
- Outlier detection (IQR method)  
- Range validation  
- Detection of non-numeric values in numeric fields  
- Extra or unexpected column detection  

### ğŸ“ˆ Streamlit Dashboard (UI)  
- Upload CSV files interactively  
- Dataset preview  
- Summary metrics  
- Color-coded **Data Quality Score**  
- Expandable detailed insights  
- Downloadable JSON report  

### ğŸ–¥ Command-Line Interface (CLI)  
For engineers and pipeline integration:

python main.py --file data/sample.csv --config config/config.yaml --out reports/report.json


### ğŸ“„ Structured JSON Reports  
All validation results are exported in an easy-to-read JSON format.

### ğŸ§± Modular Architecture  
Validator modules are cleanly separated and easy to extend.  
Configuration is handled via YAML for flexibility.

---

## ğŸ“ Project Structure

data-quality-checker/
â”‚
â”œâ”€â”€ ui/
â”‚ â””â”€â”€ app.py # Streamlit dashboard
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ validators.py # Validation logic
â”‚ â””â”€â”€ utils.py # Config & logging utilities
â”‚
â”œâ”€â”€ config/
â”‚ â””â”€â”€ config.yaml # Schema & range rules
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ sample.csv # Sample dataset
â”‚
â”œâ”€â”€ reports/
â”‚ â””â”€â”€ validation_report.json # Generated output
â”‚
â”œâ”€â”€ logs/
â”‚ â””â”€â”€ run.log # CLI logs
â”‚
â”œâ”€â”€ main.py # CLI entrypoint
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## â–¶ï¸ Run the Streamlit App

Launch the dashboard:

streamlit run ui/app.py


The browser will open automatically and display:

- Dataset preview  
- Summary metrics  
- Missing values  
- Duplicates  
- Schema issues  
- Outliers  
- Range violations  
- Data quality score  
- Download JSON button  

---

## ğŸ–¥ Running via CLI

Execute from the project root:

python main.py
--file data/sample.csv
--config config/config.yaml
--out reports/validation_report.json


The CLI generates:

- JSON report  
- Log file  
- Summary validation output  

---

## âš™ï¸ Configuration (YAML)

Located at: `config/config.yaml`

```yaml
expected_schema:
  age: int64
  salary: float64
  department: object

value_ranges:
  age: [0, 120]
  salary: [0, null]

logging:
  level: INFO

You can easily update:
Column names
Expected datatypes
Value ranges
Logging settings

##ğŸ§ª Testing With Messy Data

This tool has been validated using real messy datasets from Kaggle, including:

Adult Census Income

Titanic (raw)

Medical Appointments No-Show

House Prices

These datasets contain:

Missing values

Incorrect datatypes

Inconsistent formatting

Outliers

Extra columns

Range violations

Duplicates

Perfect for testing data quality pipelines.

ğŸ“¦ Installation

Clone the repository:
git clone https://github.com/your-username/data-quality-checker.git
cd data-quality-checker
Install dependencies:
pip install -r requirements.txt
ğŸ§° Technologies Used

Python 3

Pandas

NumPy

PyYAML

Streamlit

Logging

## ğŸŒ Deployment

Compatible with:

- **Local execution**
- **Streamlit Cloud**
- **Docker**
- **HuggingFace Spaces**
- **Heroku** (Streamlit buildpack)

The project structure is optimized for smooth deployment across platforms.

---

## ğŸ¯ Use Cases

This tool is suitable for:

- **Data validation in ML pipelines**
- **Pre-ingestion checks in ETL workflows**
- **Business data quality assessments**
- **Dashboard-based dataset audits**
- **Reproducible data integrity analysis**

---

## ğŸ‘©â€ğŸ’» Author

**Parvathy**  
Data Science â€¢ AI â€¢ Machine Learning  

This project was built to demonstrate practical data-engineering and validation skills.
